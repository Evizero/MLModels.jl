Distance-based Losses
=====================

This section lists all the subtypes of :class:`DistanceLoss`
that are implemented in this package.



LPDistLoss
-----------

.. class:: LPDistLoss

   The :math:`p`-th power absolute distance loss.
   It is Lipschitz continuous iff :math:`p = 1`, convex if and only
   if :math:`p \ge 1`, and strictly convex iff :math:`p > 1`.

+----------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------+
| Lossfunction                                                                           | Derivative                                                                             |
+========================================================================================+========================================================================================+
| .. image:: https://rawgit.com/JuliaML/FileStorage/master/LossFunctions/LPDistLoss1.svg | .. image:: https://rawgit.com/JuliaML/FileStorage/master/LossFunctions/LPDistLoss2.svg |
+----------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------+
| .. math:: L(r) = | r | ^p                                                              | .. math:: L'(r) = p \cdot r \cdot | r | ^{p-2}                                         |
+----------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------+


L1DistLoss
-----------

.. class:: L1DistLoss

   The absolute distance loss. Special case of the :class:`LPDistLoss`
   with :math:`p = 1`.
   It is Lipschitz continuous and convex, but not strictly convex.

+----------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------+
| Lossfunction                                                                           | Derivative                                                                             |
+========================================================================================+========================================================================================+
| .. image:: https://rawgit.com/JuliaML/FileStorage/master/LossFunctions/L1DistLoss1.svg | .. image:: https://rawgit.com/JuliaML/FileStorage/master/LossFunctions/L1DistLoss2.svg |
+----------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------+
| .. math:: L(r) = | r |                                                                 | .. math:: L'(r) = \textrm{sign}(r)                                                     |
+----------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------+


L2DistLoss
-----------

.. class:: L2DistLoss

   The least squares loss. Special case of the :class:`LPDistLoss`
   with :math:`p = 2`. It is strictly convex.

+----------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------+
| Lossfunction                                                                           | Derivative                                                                             |
+========================================================================================+========================================================================================+
| .. image:: https://rawgit.com/JuliaML/FileStorage/master/LossFunctions/L2DistLoss1.svg | .. image:: https://rawgit.com/JuliaML/FileStorage/master/LossFunctions/L2DistLoss2.svg |
+----------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------+
| .. math:: L(r) = | r | ^2                                                              | .. math:: L'(r) = 2 r                                                                  |
+----------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------+


LogitDistLoss
--------------

.. class:: LogitDistLoss

   The distance-based logistic loss for regression.
   It is strictly convex and Lipschitz continuous.

+-------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------+
| Lossfunction                                                                              | Derivative                                                                                |
+===========================================================================================+===========================================================================================+
| .. image:: https://rawgit.com/JuliaML/FileStorage/master/LossFunctions/LogitDistLoss1.svg | .. image:: https://rawgit.com/JuliaML/FileStorage/master/LossFunctions/LogitDistLoss2.svg |
+-------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------+
| .. math:: L(r) = - \ln \frac{4 e^r}{(1 + e^r)^2}                                          | .. math:: L'(r) = \tanh \left( \frac{r}{2} \right)                                        |
+-------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------+


HuberLoss
-----------

.. class:: HuberLoss

   .. attribute:: α

   Loss function commonly used for robustness to outliers.
   For large values of :math:`\alpha` it becomes close to the
   :class:`L1DistLoss`, while for small values of :math:`\alpha`
   it resembles the :class:`L2DistLoss`.
   It is Lipschitz continuous and convex, but not strictly convex.

+-------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Lossfunction                                                                                                                                                      | Derivative                                                                                                                                                        |
+===================================================================================================================================================================+===================================================================================================================================================================+
| .. image:: https://rawgit.com/JuliaML/FileStorage/master/LossFunctions/HuberLoss1.svg                                                                             | .. image:: https://rawgit.com/JuliaML/FileStorage/master/LossFunctions/HuberLoss2.svg                                                                             |
+-------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| .. math:: L(r) = \begin{cases} \frac{r^2}{2} & \quad \text{if } | r | \le \alpha \\ \alpha | r | - \frac{\alpha^2}{2} & \quad \text{otherwise}\\ \end{cases}      | .. math:: L'(r) = \begin{cases} r & \quad \text{if } | r | \le \alpha \\ \alpha \cdot \textrm{sign}(r) & \quad \text{otherwise}\\ \end{cases}                     |
+-------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------+


L1EpsilonInsLoss
-----------------

.. class:: L1EpsilonInsLoss

   .. attribute:: ϵ

   The :math:`\epsilon`-insensitive loss. Typically used in linear
   support vector regression. It ignores deviances smaller than
   :math:`\epsilon` , but penalizes larger deviances linarily.
   It is Lipschitz continuous and convex, but not strictly convex.

+---------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------+
| Lossfunction                                                                                                                          | Derivative                                                                                                                           |
+=======================================================================================================================================+======================================================================================================================================+
| .. image:: https://rawgit.com/JuliaML/FileStorage/master/LossFunctions/L1EpsilonInsLoss1.svg                                          | .. image:: https://rawgit.com/JuliaML/FileStorage/master/LossFunctions/L1EpsilonInsLoss2.svg                                         |
+---------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------+
| .. math:: L(r) = \max \{ 0, | r | - \epsilon \}                                                                                       | .. math:: L'(r) = \begin{cases} \frac{r}{ | r | } & \quad \text{if } \epsilon \le | r | \\ 0 & \quad \text{otherwise}\\ \end{cases}  |
+---------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------+


L2EpsilonInsLoss
-----------------

.. class:: L2EpsilonInsLoss

   .. attribute:: ϵ

   The :math:`\epsilon`-insensitive loss. Typically used in linear
   support vector regression. It ignores deviances smaller than
   :math:`\epsilon` , but penalizes larger deviances quadratically.
   It is convex, but not strictly convex.

+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Lossfunction                                                                                                                                                                      | Derivative                                                                                                                                                                        |
+===================================================================================================================================================================================+===================================================================================================================================================================================+
| .. image:: https://rawgit.com/JuliaML/FileStorage/master/LossFunctions/L2EpsilonInsLoss1.svg                                                                                      | .. image:: https://rawgit.com/JuliaML/FileStorage/master/LossFunctions/L2EpsilonInsLoss2.svg                                                                                      |
+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| .. math:: L(r) = \max \{ 0, | r | - \epsilon \}^2                                                                                                                                 | .. math:: L'(r) = \begin{cases} 2 \cdot \textrm{sign}(r) \cdot \left( | r | - \epsilon \right) & \quad \text{if } \epsilon \le | r | \\ 0 & \quad \text{otherwise}\\ \end{cases}  |
+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+


PeriodicLoss
-------------

.. class:: PeriodicLoss

   .. attribute:: c

   Measures distance on a circle of specified circumference :math:`c`.

+-----------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+
| Lossfunction                                                                                  | Derivative                                                                                    |
+===============================================================================================+===============================================================================================+
| .. image:: https://rawgit.com/JuliaML/FileStorage/master/LossFunctions/PeriodicLoss1.svg      | .. image:: https://rawgit.com/JuliaML/FileStorage/master/LossFunctions/PeriodicLoss2.svg      |
+-----------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+
| .. math:: L(r) = 1 - \cos \left ( \frac{2 r \pi}{c} \right )                                  | .. math:: L'(r) = \frac{2 \pi}{c} \cdot \sin \left( \frac{2r \pi}{c} \right)                  |
+-----------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+


QuantileLoss
-------------

.. class:: QuantileLoss

   .. attribute:: τ

    The quantile loss, aka pinball loss. Typically used to estimate
    the conditional :math:`\tau`-quantiles.
    It is convex, but not strictly convex. Furthermore it is
    Lipschitz continuous.

+------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------+
| Lossfunction                                                                                                                             | Derivative                                                                                                                               |
+==========================================================================================================================================+==========================================================================================================================================+
| .. image:: https://rawgit.com/JuliaML/FileStorage/master/LossFunctions/QuantileLoss1.svg                                                 | .. image:: https://rawgit.com/JuliaML/FileStorage/master/LossFunctions/QuantileLoss2.svg                                                 |
+------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------+
| .. math:: L(r) = \begin{cases} \left( 1 - \tau \right) r & \quad \text{if } r \ge 0 \\ - \tau r & \quad \text{otherwise} \\ \end{cases}  | .. math:: L(r) = \begin{cases} 1 - \tau & \quad \text{if } r \ge 0 \\ - \tau & \quad \text{otherwise} \\ \end{cases}                     |
+------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------+

.. note::

   You may note that our definition of the QuantileLoss looks
   different to what one usually sees in other literature.
   The reason is that we have to correct for the fact that in our
   case :math:`r = \hat{y} - y` instead of
   :math:`r_{\textrm{usual}} = y - \hat{y}`, which means that our
   definition relates to that in the manner of
   :math:`r = -1 * r_{\textrm{usual}}`.

